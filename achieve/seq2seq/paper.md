# 论文翻译与心得

存放对这篇论文的中文翻译，方便阅读。

论文地址：
**[Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)**



##  **1 Introduction**

深度神经网络 （DNN） 是极其强大的机器学习模型，可在语音识别 [13， 7] 和视觉对象识别 [19， 6， 21， 20] 等难题上取得优异的性能。DNN 功能强大，因为它们可以对适度的步骤执行任意并行计算。DNN 强大的一个令人惊讶的例子是它们能够仅使用 2 个二次大小的隐藏层对 N N 位数字进行排序 [27]。因此，虽然神经网络与传统统计模型相关，但它们学习的是复杂的计算。此外，只要标记的训练集有足够的信息来指定网络的参数，大型 DNNscan就会使用监督式反向传播进行训练。因此，如果存在一个大型 DNN 的参数设置，该设置可以获得良好的结果（例如，因为人类可以非常快速地解决任务），则监督反向传播将找到这些参数并解决问题。

尽管 DNN 具有灵活性和强大功能，但它们只能应用于其输入和目标可以使用固定维度向量进行合理编码的问题。这是一个很大的局限性，因为许多重要问题最好用长度不先验已知的序列来表达。例如，语音识别和机器翻译是顺序问题。同样，问题回答也可以看作是将代表问题的单词序列映射到代表答案的 1 个单词序列。因此，很明显，一种学习将序列映射到序列的独立于域的方法将是有用的。

序列对 DNN 提出了挑战，因为它们要求输入和输出的维数是已知且固定的。在本文中，我们展示了长短期记忆 （LSTM） 架构 [16] 的直接应用可以解决一般的序列到序列问题。这个想法是使用一个 LSTM 来读取输入序列，一次一个时间步，以获得大的固定维向量表示，然后使用另一个 LSTM 从该向量中提取输出序列（图 1）。第二个 LSTM 本质上是一个递归神经网络语言模型 [28， 23， 30]，只是它以输入序列为条件。LSTM 能够成功学习具有长距离时间依赖性的数据，这使其成为该应用的自然选择，因为输入与其相应的输出之间存在相当大的时间滞后（图 1）。

已经有许多相关的尝试来解决神经网络的一般序列到序列学习问题。我们的方法与 Kalchbrenner 和 Blunsom [18] 密切相关，他们是第一个将整个输入句子映射到向量的人，并且与 Cho 等人 [5] 非常相似。Graves [10] 引入了一种新的可微分注意力机制，允许神经网络专注于其输入的不同部分，Bahdanau 等人 [2] 成功地将这一想法的一个优雅变体应用于机器翻译。连接主义序列分类是另一种流行的技术，用于使用神经网络将序列映射到序列，尽管它假设输入和输出之间是单调对齐的 [11]

图 1：我们的模型读取输入句子 “ABC” 并生成 “WXYZ” 作为输出句子。该模型在输出句尾标记后停止进行预测。请注意，LSTM 会反向读取输入句子，因为这样做会在数据中引入许多短期依赖关系，从而使优化问题变得更加容易

这项工作的主要结果如下。在 WMT'14 英译法翻译任务中，我们通过使用简单的从左到右的波束搜索解码器直接从 5 个深度 LSTM 的集合中提取翻译（每个 LSTM 具有 380M 参数），获得了 34.81 的 BLEU 分数。这是迄今为止使用大型神经网络进行直接翻译所获得的最佳结果。相比之下，该数据集上 SMT 基线的 BLEU 评分为 33.30 [29]。34.81 BLEU 分数是由词汇量为 80k 个单词的 LSTM 获得的，因此每当参考翻译包含这 80k 个单词未涵盖的单词时，该分数就会被扣分。这一结果表明，相对未优化的神经网络架构具有很大的改进空间，其性能优于成熟的基于短语的 SMT 系统

最后，我们使用 LSTM 对同一任务中公开可用的 1000 个 SMT 基线列表进行重新评分 [29]。通过这样做，我们获得了 36.5 的 BLEU 分数，将基线提高了 3.2 BLEU 分，接近之前的最新水平（37.0 [9]）。

令人惊讶的是，LSTM 并没有在很长的句子中受到影响，尽管最近其他人的经验 具有相关架构的研究人员[26]。**我们能够在长句子上表现出色，因为我们 在训练和测试中颠倒了源句子中的单词顺序，但没有颠倒目标句子中的单词顺序放**。通过这样做，我们引入了许多短期依赖性，从而导致优化问题简单得多（参见第 2 节和第 3.3 节）。因此，SGD 可以学习没有任何问题的 LSTM长句子。颠倒源句中的单词的简单技巧是关键之一这项工作的技术贡献。

LSTM 的一个有用特性是它学习将可变长度的输入句子映射到固定维度向量表示。鉴于翻译往往是源句子的释义，翻译目标鼓励 LSTM 找到能够捕捉其含义的句子表示，因为具有相似含义的句子彼此接近，而不同的 2 个句子含义会很远。定性评估支持这一说法，表明我们的模型知道词序，

##  2 The model

递归神经网络 （RNN） [31， 28] 是前馈神经网络对序列的自然泛化。给定一个输入序列 （x1,...,xT），标准 RNN 通过迭代以下方程来计算输出序列 （y1,...,yT）

只要提前知道输入和输出之间的对齐情况，RNN 就可以轻松地将序列映射到序列。然而，目前尚不清楚如何将 RNN 应用于输入和输出序列具有不同长度的问题，以及复杂和非单调的关系

一般序列学习的一种简单策略是使用一个 RNN 将输入序列映射到固定大小的向量，然后使用另一个 RNN 将向量映射到目标序列（Cho 等人 [5] 也采用了这种方法）。虽然它原则上可以工作，因为RNN已经提供了所有相关信息，但训练RNN是困难的由于它产生的长期依赖性问题[14,4]（图1）[16,15]。然而，众所周知，长短期记忆 （LSTM） [16] 会学习长距离时间依赖性问题，因此 LSTM 可能会在这种设置下取得成功。

LSTM 的目标是估计条件概率 p（y1,...,yT′|x1,...,xT），其中 （x1,...,xT） 是一个输入序列，y1,...,yT′ 是其相应的输出序列，其长度 T′ 可能与 T 不同。LSTM 通过首先获得由 LSTM 的最后一个隐藏状态给出的输入序列 （x1,...,xT） 的固定维表示 v，然后使用初始隐藏状态设置为 x1,...,xT 的表示 v 的标准 LSTM-LM 公式计算 y1,...,yT' 的概率

在这个方程中，每个 p（yt|v，y1,...,yt−1） 分布都用词汇表中所有单词的 softmax 表示。我们使用 Graves [10] 的 LSTM 公式。请注意，我们要求每个句子都以一个特殊的句尾符号 “<EOS>” 结尾，这使模型能够定义所有可能长度的序列的分布。图 1 概述了整个方案，其中所示的 LSTM 计算 “A”、“B”、“C”、“” 的表示形式<EOS>，然后使用此表示形式计算 “W”、“X”、“Y”、“Z”、“” 的概率<EOS>

我们的实际模型在三个重要方面与上述描述不同。首先，我们使用了两种不同的 LSTM：一种用于输入序列，另一种用于输出序列，因为这样做会增加数量模型参数，而计算成本可以忽略不计，并且使得同时在多个语言对上训练 LSTM 变得很自然 [18]。其次，我们发现深 LSTM 的性能明显优于浅层 LSTM，因此我们选择了具有四层的 LSTM。第三，我们发现颠倒输入句子的单词顺序非常有价值。因此，例如，不是将句子 a，b，c 映射到句子 α，β，γ，而是要求 LSTM 将 c，b，a 映射到 α，β，γ，其中 α，β，γ 是 a，b，c 的翻译。这样，a 与 α 非常接近，b 与 β 相当接近，依此类推，这一事实使 SGD 很容易在输入和输出之间“建立通信”。我们发现这种简单的数据转换可以大大提高 LSTM 的性能

##  3 Experiments

我们以两种方式将我们的方法应用于 WMT'14 英语到法语 MT 任务。我们用它来直接翻译输入句子，而无需使用参考 SMT 系统，并且我们用它来重新评分 SMT 基线的 n 个最佳列表。我们报告了这些翻译方法的准确性，提供了示例翻译，并将生成的句子表示可视化。

### 3.1 Dataset details

我们使用了 WMT'14 英语到法语数据集。我们在由 348M 法语单词和 304M 英语单词组成的 12M 语义子集上训练了我们的模型，这是 [29] 中一个干净的“选定”子集。我们选择这个翻译任务和这个特定的训练集子集，是因为标记化的训练和测试集以及来自基线 SMT 的 1000 个最佳列表的公开可用性[29]。

由于典型的神经语言模型依赖于每个单词的向量表示，因此我们对两种语言都使用了固定的词汇表。我们对源语言使用了 160,000 个最常用的单词，为目标语言使用了 80,000 个最常用的单词。每个词汇外的单词都被替换为一个特殊的 “UNK” 标记。

###  3.2 Decoding and Rescoring

我们实验的核心涉及在许多句子对上训练一个大型深度 LSTM。我们通过在给定源句子 S 的情况下最大化正确翻译 T 的对数概率来训练它，因此训练目标是

其中 S 是训练集。训练完成后，我们根据 LSTM 找到最可能的翻译来生成翻译：

我们使用一个简单的从左到右的光束搜索解码器来搜索最可能的翻译，该解码器维护少量的部分假设 B，其中部分假设是某些翻译的前缀。在每个时间步长，我们用词汇表中的每一个可能的单词扩展光束中的每个部分假设。这大大增加了假设的数量，因此我们根据模型的对数概率丢弃除 B 最可能的假设之外的所有假设。一旦 “<EOS>” 符号附加到假设验证，它就会从梁中删除并添加到完整假设的集合中。虽然此解码器是近似的，但易于实现。有趣的是，即使光束尺寸为 1，我们的系统也表现良好，而光束尺寸为 2 提供了光束搜索的大部分好处（表 1）。

我们还使用 LSTM 对基线系统生成的 1000 个最佳列表进行重新评分 [29]。为了对 n-best 列表重新评分，我们使用 LSTM 计算了每个假设的对数概率，并对其分数和 LSTM 的分数取偶数。

###  3.3 Reversing the Source Sentences

虽然 LSTM 能够解决具有长期依赖关系的问题，但我们发现，当源句子反转时（目标句子没有反转），LSTM 的学习效果要好得多。这样一来，LSTM 的测试困惑度从 5.8 下降到 4.7，其解码翻译的测试 BLEU 分数从 25.9 提高到 30.6。

虽然我们没有对这种现象的完整解释，但我们认为这是由于数据集引入了许多短期依赖关系造成的。通常，当我们将源句子与目标句子连接起来时，源句子中的每个单词都与目标句子中的对应单词相距甚远。因此，该问题具有很大的“最小时间滞后”[17]。通过反转源句子中的单词，源语言和目标语言中对应单词之间的平均距离保持不变。但是，源语言中的前几个单词现在与目标语言中的前几个单词非常接近，因此问题的最小时间滞后大大减少。因此，反向传播更容易在源句子和目标句子之间 “建立通信”，这反过来又大大提高了整体性能。

最初，我们认为颠倒输入句子只会导致目标句子的早期部分的预测更有信心，而后期的预测则不那么自信。然而，在反向源句子上训练的 LSTM 在长句子上的表现比在原始源句子上训练的 LSTM 4 要好得多（参见第 3.7 节），这表明反转输入句子会导致 LSTM 具有更好的内存利用率。

### 3.4 Training details

我们发现 LSTM 模型相当容易训练。我们使用了 4 层的深度 LSTM，每层有 1000 个单元格和 1000 个维度词嵌入，输入词汇量为 160,000 个，输出词汇量为 80,000 个。我们发现深度 LSTM 的性能明显优于浅层 LSTM，浅层每增加一层，困惑度就会降低近 10%，这可能是因为它们的隐藏状态要大得多。我们在每个输出中使用了超过 80,000 个单词的 naive softmax。生成的 LSTM 具有 380M 参数，其中 64M 是纯循环连接（“编码器”LSTM为 32M，“解码器”LSTM 为 32M）。完整的训练细节如下：

-  我们初始化了 LSTM 的所有参数，其均匀分布在 -0.08 和 0.08 之间 
- 我们使用了无动量的随机梯度下降，固定学习率为 0.7。在 5 个 epoch 之后，我们开始将每半个 epoch 的学习率减半。我们总共训练了 7.5 个 epoch 的模型
- 我们使用 128 个序列的批次进行梯度，并将其除以批次的大小（即 128）
- 尽管 LSTM 往往不会遇到梯度消失问题，但它们可能会出现梯度爆炸。因此，我们对梯度的范数 [10， 25] 强制执行了硬约束，当其范数超过阈值时对其进行缩放。对于每个训练批次，我们计算 s = g 2，其中 g 是梯度除以 128。如果 s > 5，则设置 g = 5g s 
- 不同的句子有不同的长度。大多数句子很短（例如，长度为 20-30），但有些句子很长（例如，长度> 100），因此随机选择的 128 个训练句子的小批量将包含许多短句和很少的长句，因此，小批量中的大部分计算都被浪费了。为了解决这个问题，我们确保一个小批量中的所有句子的长度大致相同，这加快了 2 倍

## 心得

